{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Thu Nov 2 11:19:21 2021\n",
    "@author: cliu\n",
    "\"\"\"\n",
    "import os\n",
    "import yaml\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bisect import *\n",
    "import random\n",
    "import ast\n",
    "_join = os.path.join\n",
    "\n",
    "CONFIG = r'T:/ABM/ABM_FY22/RSM/VisionEval/Model/Update_Automations/settings.yml'\n",
    "with open(CONFIG) as cff:\n",
    "    config =yaml.safe_load(cff)\n",
    "driverHTSBin = config['driverHTSBin']\n",
    "scenpath = ''.join(config['scenpath'])\n",
    "target_path = ''.join(config['target_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 ToursByHh_df.csv\n",
    "df_tour = pd.read_csv(_join('HTS', 'tours_debug_newid.csv'))\n",
    "df_hh = pd.read_csv(_join('HTS', 'SDRTS_Household_Data_20170731 TAZ.csv'))\n",
    "df_trip_hts = pd.read_csv(_join('HTS', 'SDRTS_Trip_Data_20170731 TAZ.csv'))[['hhid', 'pernum', 'tripnum','travelers_total','mode1']] \n",
    "df_trip_rsg = pd.read_csv(_join('HTS', 'trips_debug_newid.csv'))[['HH_ID','PER_ID','TOUR_ID','TRIP_ID', 'ORIG_PLACENO', 'TRIPMODE','puid', 'mode_adj']]\n",
    "df_veh_hts = pd.read_csv(_join('HTS', 'SDRTS_Vehicle_Data_20170731(2016HTS).csv'))[['hhid', 'vehnum', 'category']]\n",
    "df_mode = pd.read_csv(_join('NHTS2001 analysis', 'mode share.csv'))\n",
    "\n",
    "df_tour = df_tour.merge(df_hh[['hhid', 'home_address_TAZ']], left_on='HH_ID', right_on='hhid', how=\"left\")\n",
    "df_tour = df_tour.fillna(0)\n",
    "\n",
    "# estimating TravelTime, DwellTime, StartHome, EndHome\n",
    "df_tour['DwellTime'] =  df_tour['OSTOP_1_DUR_HR']*60 + df_tour['OSTOP_1_DUR_MIN'] + \\\n",
    "                        df_tour['OSTOP_2_DUR_HR']*60 + df_tour['OSTOP_2_DUR_MIN'] + \\\n",
    "                        df_tour['OSTOP_3_DUR_HR']*60 + df_tour['OSTOP_3_DUR_MIN'] + \\\n",
    "                        df_tour['OSTOP_4_DUR_HR']*60 + df_tour['OSTOP_4_DUR_MIN'] + \\\n",
    "                        df_tour['ISTOP_1_DUR_HR']*60 + df_tour['ISTOP_1_DUR_MIN'] + \\\n",
    "                        df_tour['ISTOP_2_DUR_HR']*60 + df_tour['ISTOP_2_DUR_MIN'] + \\\n",
    "                        df_tour['ISTOP_3_DUR_HR']*60 + df_tour['ISTOP_3_DUR_MIN'] + \\\n",
    "                        df_tour['ISTOP_4_DUR_HR']*60 + df_tour['ISTOP_4_DUR_MIN'] + \\\n",
    "                        df_tour['PRIMDEST_DEPART_HOUR']*60 + df_tour['PRIMDEST_DEPART_MIN'] - \\\n",
    "                        df_tour['PRIMDEST_ARRIVE_HOUR']*60 - df_tour['PRIMDEST_ARRIVE_MIN']\n",
    "df_tour['TravelTime'] = df_tour['TOUR_DUR_HR']*60 + df_tour['TOUR_DUR_MIN'] - df_tour['DwellTime']  \n",
    "df_tour.loc[df_tour['home_address_TAZ'] == df_tour['OTAZ'], 'StartHome'] = 'TRUE'\n",
    "df_tour['StartHome'] = df_tour['StartHome'].fillna('FALSE')\n",
    "df_tour.loc[df_tour['home_address_TAZ'] == df_tour['DTAZ'], 'EndHome'] = 'TRUE'\n",
    "df_tour['EndHome'] = df_tour['EndHome'].fillna('FALSE')\n",
    "\n",
    "# estimating trips and persons (travelers_total)\n",
    "df_trip_rsg = df_trip_rsg.merge(df_trip_hts, left_on=['HH_ID','PER_ID','ORIG_PLACENO'], \\\n",
    "                                right_on=['hhid', 'pernum', 'tripnum'], how='left')\n",
    "df_trip_rsg2 = df_trip_rsg.groupby(['HH_ID','PER_ID','TOUR_ID'])['TRIP_ID', 'travelers_total'].max().reset_index()\n",
    "df_trip_rsg2.rename(columns={'TRIP_ID':'Trips'}, inplace=True) \n",
    "df_tour2 = df_tour.merge(df_trip_rsg2[['HH_ID','PER_ID','TOUR_ID','Trips','travelers_total']],\\\n",
    "                         on=['HH_ID','PER_ID','TOUR_ID'], how=\"left\")\n",
    "df_tour2 = df_tour2.merge(df_trip_rsg[['HH_ID','PER_ID','TOUR_ID','TRIP_ID','mode1','mode_adj']], \n",
    "                          on=['HH_ID','PER_ID','TOUR_ID'], how=\"left\")\n",
    "df_tour2 = df_tour2[df_tour2['TRIP_ID'] == 1] # only keeps the first trip leg for each tour\n",
    "\n",
    "# estimating Vehtype, HhVehUsed\n",
    "df_tour2.loc[(df_tour2['TOURMODE']<=3) & (df_tour2['mode1']>=6) & (df_tour2['mode1']<=16), 'vehnum'] = df_tour2['mode1']-5\\\n",
    "                            # auto mode to Auto, LtTrk, OtherTrk, Motorcycle. TOURMODE-HTS, mode1-HTS trip mode(first mode)\n",
    "df_tour2 = df_tour2.merge(df_veh_hts, on=['hhid','vehnum'], how='left')\n",
    "df_tour2['Vehtype'] = df_tour2['category'].replace({'car': 1, 'light truck': 4, 'Motorcycle': 7, 'OthTrk': 5})\n",
    "df_tour2.loc[(df_tour2['TOURMODE']<=3) , 'HhVehUsed'] = 2\n",
    "df_tour2.loc[(df_tour2['TOURMODE']<=3) & (df_tour2['vehnum']>0), 'HhVehUsed'] = 1\n",
    "\n",
    "# converting HTS tour mode to NHTS tour mode (Trptrans)\n",
    "df_tour2['Trptrans'] = df_tour2 ['TOURMODE']\n",
    "df_tour2['Trptrans'] = df_tour2['Trptrans'].replace({10:22})\n",
    "df_tour2['Trptrans'] = df_tour2['Trptrans'].replace({2: 1, 3: 1, 4: 26, 5: 25, 6: 10, 7: 10, 8: 10, 9: 10, 11: 12})\n",
    "df_tour2.loc[(df_tour2['Vehtype']>0), 'Trptrans'] = df_tour2['Vehtype'] # update auto trip modes if household vehicle is used\n",
    "# 39-LRT:18-StreetCar, 41-RailIntercity/42-Rail Other:15-Train\n",
    "df_tour2.loc[(df_tour2['mode_adj']==39) & (df_tour2['Trptrans']==10), 'Trptrans'] = 18\n",
    "df_tour2.loc[(df_tour2['mode_adj']==41) & (df_tour2['Trptrans']==10), 'Trptrans'] = 15\n",
    "df_tour2.loc[(df_tour2['mode_adj']==42) & (df_tour2['Trptrans']==10), 'Trptrans'] = 15\n",
    "df_tour2.loc[(df_tour2['mode_adj']==150) & (df_tour2['Trptrans']==10), 'Trptrans'] = 16\n",
    "\n",
    "df_tour2['Vehtype'] = df_tour2['Vehtype'].fillna('NA')\n",
    "df_tour2['HhVehUsed'] = df_tour2['HhVehUsed'].fillna('NA')\n",
    "\n",
    "# Approximate Vehid, Disttowk, Whyto - these attributes are not in used in VERSPM modules\n",
    "df_tour2['Vehid'] = 0\n",
    "df_tour2['Disttowk'] = 0\n",
    "df_tour2['Whyto'] ='1-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Imputation of auto tour mode if household vehichle is not used (auto, light track, other truck, RV, motorcycel)\n",
    "df_tour2['IsDist20'] = '2'\n",
    "df_tour2.loc[df_tour2['DIST']<=20, 'IsDist20'] = '1'\n",
    "#df_tour2['IsTrips2'] = '2'\n",
    "#df_tour2.loc[df_tour2['Trips']<=2, 'IsTrips2'] = '1'\n",
    "df_tour2['IsPerson1'] = '2'\n",
    "df_tour2.loc[df_tour2['travelers_total']<=1, 'IsPerson1'] = '1'\n",
    "df_tour2['IsWorkTour'] = '2'\n",
    "df_tour2.loc[df_tour2['TOURPURP']==1, 'IsWorkTour'] = '1'\n",
    "df_tour2.loc[df_tour2['TOURPURP']==1, 'Whyto'] = '11-11'\n",
    "df_tour2['condition'] =  df_tour2[['IsDist20', 'IsPerson1','IsWorkTour']].agg('-'.join, axis=1)\n",
    "df_tour2 = df_tour2.merge(df_mode[['condition','probability']], on='condition', how='left')\n",
    "for i in range(len(df_tour2)): # convert property list from string\n",
    "    df_tour2['probability'][i] = ast.literal_eval(df_tour2['probability'][i])\n",
    "df_tour2['bisect'] = 0\n",
    "for i in range(len(df_tour2)): # imputation\n",
    "    df_tour2['bisect'][i] = bisect(df_tour2['probability'][i], random.random()) + 1\n",
    "df_tour2['bisect'] = df_tour2['bisect'].replace(3,5)\n",
    "df_tour2.loc[df_tour2['HhVehUsed'] == 2, 'Trptrans'] = df_tour2['bisect']\n",
    "df_tour2.loc[df_tour2['HhVehUsed'] == 2, 'Vehtype'] = df_tour2['Trptrans']\n",
    "# output\n",
    "df = df_tour2[['HH_ID','DIST','TravelTime','DwellTime','StartHome', 'EndHome','Trips','travelers_total',\\\n",
    "               'Vehid','Trptrans','Vehtype','HhVehUsed','Whyto','Disttowk']]\n",
    "df.columns = ['Houseid','Distance','TravelTime','DwellTime','StartHome','EndHome','Trips','Persons','Vehid',\\\n",
    "              'Trptrans','Vehtype','HhVehUsed','Whyto','Disttowk']\n",
    "# df['Houseid']  = df['Houseid'].apply(lambda x: \"'\" + str(x) + \"'\")\n",
    "list = [['Disttowk',float]]\n",
    "for item, format in list:\n",
    "    df[item] = df[item].astype(format)\n",
    "df.to_csv('ToursByHh_df.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 3 Per_df.csv\n",
    "df_per = pd.read_csv(_join('HTS', 'per_debug.csv'))[['SAMPN', 'personid', 'PERNO', 'gender', 'AGE_CAT', 'EMPLY', 'license','HHR_RACE']]\n",
    "df_personABM = pd.read_csv(_join(scenpath, '2016', 'input\\persons.csv'))[['perid','age']]\n",
    "df_per['WORKER'] = 2\n",
    "df_per.loc[df_per['EMPLY']<=2, 'WORKER'] = 1\n",
    "df_per['license'] = df_per['license'].fillna(2)\n",
    "# Assign age group. HTS age group is different to VE age group. Approximate it with the age group proportion in ABM persons.csv\n",
    "labels = [15, 24, 34,54, 64, 80]\n",
    "df_per['personBins'] = pd.cut(df_per['AGE_CAT'], bins=driverHTSBin, labels=labels, include_lowest=True)\n",
    "df_perGro = df_per.groupby(['personBins'])['personid'].count().reset_index(name ='Sum')\n",
    "age_bin2 = [0,5,14,15,17,18,19,24,29,34,54,64,150] \n",
    "age_labels = ['0-5','6-14','15','16-17','18','19','20-24','25-29','30-34','35-54','55-64','65+']\n",
    "df_personABM['personBins'] = pd.cut(df_personABM['age'], bins=age_bin2, labels=age_labels , include_lowest=True)\n",
    "df_personABM2 = df_personABM.groupby(['personBins'])['perid'].count().reset_index(name ='Sum')\n",
    "personABM_list = df_personABM2['Sum'].to_list()\n",
    "personABMper_list = [] # abm age proportation [0-14, 15, 16, 17, 18, 19,20-24, 25-29, 30-34]\n",
    "personABMper_list.append(personABM_list[0]/(personABM_list[0]+personABM_list[1]+personABM_list[2]))\n",
    "personABMper_list.append(personABM_list[1]/(personABM_list[0]+personABM_list[1]+personABM_list[2]))\n",
    "personABMper_list.append(personABM_list[2]/(personABM_list[0]+personABM_list[1]+personABM_list[2]))\n",
    "personABMper_list.append(personABM_list[3]/(personABM_list[3]+personABM_list[4]+personABM_list[5]+personABM_list[6]))\n",
    "personABMper_list.append(personABM_list[4]/(personABM_list[3]+personABM_list[4]+personABM_list[5]+personABM_list[6]))\n",
    "personABMper_list.append(personABM_list[5]/(personABM_list[3]+personABM_list[4]+personABM_list[5]+personABM_list[6]))\n",
    "personABMper_list.append(personABM_list[6]/(personABM_list[3]+personABM_list[4]+personABM_list[5]+personABM_list[6]))\n",
    "personABMper_list.append(personABM_list[7]/(personABM_list[7]+personABM_list[8]))\n",
    "personABMper_list.append(personABM_list[8]/(personABM_list[7]+personABM_list[8]))\n",
    "df_per['R_AGE'] = 0\n",
    "for i in range(len(df_per)): # imputation to more age bins for Hh_df LIF_CYC\n",
    "    if df_per['personBins'][i] == 15:\n",
    "        value = bisect([personABMper_list[0],personABMper_list[0]+personABMper_list[1] ,1], random.random())\n",
    "        if value == 0:\n",
    "            df_per['R_AGE'][i] = 5\n",
    "        elif value == 1:\n",
    "            df_per['R_AGE'][i] = 14                                             \n",
    "        else: \n",
    "            df_per['R_AGE'][i] = 15\n",
    "    elif df_per['personBins'][i] == 24:\n",
    "        value = bisect([personABMper_list[3],personABMper_list[3]+personABMper_list[4],\\\n",
    "                        personABMper_list[3]+personABMper_list[4]+personABMper_list[5], 1], random.random())\n",
    "        if value == 0:\n",
    "            df_per['R_AGE'][i] = 16\n",
    "        elif value == 1:\n",
    "            df_per['R_AGE'][i] = 18\n",
    "        elif value == 2:\n",
    "            df_per['R_AGE'][i] = 19\n",
    "        else: \n",
    "            df_per['R_AGE'][i] = 24\n",
    "    elif df_per['personBins'][i] == 34:\n",
    "        value = bisect([personABMper_list[7],1], random.random())\n",
    "        if value == 0:\n",
    "            df_per['R_AGE'][i] = 29\n",
    "        else: \n",
    "            df_per['R_AGE'][i] = 34      \n",
    "    else:\n",
    "        df_per['R_AGE'][i] = df_per['personBins'][i]\n",
    "# daily number of walk and bike trips\n",
    "df_trip_rsg['day'] = df_trip_rsg['puid'].str[-1:].astype(int) \n",
    "trip_rsg_1day = df_trip_rsg.loc[(df_trip_rsg['day'] == 1) & (df_trip_rsg['TRIPMODE'] == 4)]\n",
    "trip_rsg_1day_group = trip_rsg_1day.groupby(['HH_ID', 'PER_ID'])['TRIP_ID'].count().reset_index().rename(columns={'TRIP_ID': 'NWALKTRP'}) \n",
    "df_per = df_per.merge(trip_rsg_1day_group, left_on=['SAMPN', 'PERNO'], right_on=['HH_ID', 'PER_ID'], how='left')\n",
    "df_per['NWALKTRP'] = df_per['NWALKTRP'].fillna(0)\n",
    "df_per['NWALKTRP'] = df_per['NWALKTRP'] * 7 * 1.06\n",
    "trip_rsg_1dayb = df_trip_rsg.loc[(df_trip_rsg['day'] == 1) & (df_trip_rsg['TRIPMODE'] == 5)]\n",
    "trip_rsg_1day_groupb = trip_rsg_1dayb.groupby(['HH_ID', 'PER_ID'])['TRIP_ID'].count().reset_index().rename(columns={'TRIP_ID': 'NBIKETRP'}) \n",
    "df_per = df_per.merge(trip_rsg_1day_groupb, left_on=['SAMPN', 'PERNO'], right_on=['HH_ID', 'PER_ID'], how='left')\n",
    "df_per['NBIKETRP'] = df_per['NBIKETRP'].fillna(0)\n",
    "df_per['NBIKETRP'] = df_per['NBIKETRP'] * 7 * 1.06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.5 Imputation of weekly bike trips per person\n",
    "'''\n",
    "df_bike = pd.read_csv(_join('NHTS2001 analysis', 'bike share.csv'))\n",
    "df_per['R_AGE_Con'] = '0'\n",
    "df_per.loc[df_per['R_AGE']<=35, 'R_AGE_Con'] = '1'\n",
    "df_per['WORKERs'] = df_per['WORKER'].astype(str)\n",
    "df_per['licenses'] = df_per['license'].astype(int).astype(str)\n",
    "df_per['genders'] = df_per['gender'].astype(str)\n",
    "df_per['condition'] =  df_per[['WORKERs', 'licenses','genders','R_AGE_Con']].agg('-'.join, axis=1)\n",
    "df_per = df_per.merge(df_bike[['condition','probability']], on='condition', how='left')\n",
    "for i in range(len(df_per)): # convert property list from string\n",
    "    df_per['probability'][i] = ast.literal_eval(df_per['probability'][i])\n",
    "df_per['NBIKETRP'] = 0\n",
    "for i in range(len(df_per)): # imputation\n",
    "    if df_per['R_AGE'][i] >= 16:\n",
    "        df_per['NBIKETRP'][i] = bisect(df_per['probability'][i], random.random())\n",
    "df_per['NBIKETRP'] = df_per['NBIKETRP'].replace({3:4, 4:14})\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.6 Approximate COMMDRVR, USEPUBTR, WRKDRIVE, DTGAS, DISTTOWK - these attributes are not in used in VERSPM modules\n",
    "df_per['COMMDRVR'] =-1\n",
    "df_per['USEPUBTR'] = -1\n",
    "df_per['WRKDRIVE'] = -1\n",
    "df_per['WRKTRANS'] = -1\n",
    "df_per['DTGAS'] = -1\n",
    "df_per['DISTTOWK'] = 0\n",
    "df_per2 = df_per[['SAMPN','personid','COMMDRVR','NBIKETRP','NWALKTRP','USEPUBTR','WRKDRIVE','WRKTRANS','WORKER',\\\n",
    "             'DTGAS','DISTTOWK','license','R_AGE','gender']]\n",
    "df_per2.columns = ['HOUSEID','PERSONID','COMMDRVR','NBIKETRP','NWALKTRP','USEPUBTR','WRKDRIVE','WRKTRANS','WORKER',\\\n",
    "              'DTGAS','DISTTOWK','DRIVER','R_AGE','R_SEX']\n",
    "# df_per2['HOUSEID']  = df_per2['HOUSEID'].apply(lambda x: \"'\" + str(x) + \"'\")\n",
    "list = [['PERSONID',int],['DISTTOWK',float],['DRIVER',int]]\n",
    "for item, format in list:\n",
    "    df_per2[item] = df_per2[item].astype(format)\n",
    "df_per2.to_csv('Per_df.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 Veh_df\n",
    "df_veh = pd.read_csv(_join('HTS', 'SDRTS_Vehicle_Data_20170731(2016HTS).csv'))[['hhid', 'vehnum', 'year', 'category']]\n",
    "df_veh = df_veh.rename(columns={'hhid':'HOUSEID', 'vehnum':'VEHID', 'year':'VEHYEAR', 'category':'VEHTYPE'})\n",
    "df_veh['VEHTYPE'] = df_veh['VEHTYPE'].replace({'car': 1, 'light truck': 4, 'Motorcycle': 7, 'OthTrk': 5})\n",
    "df_veh['BESTMILE'] = -9\n",
    "df_veh['EIADMPG'] = -9\n",
    "df_veh['GSCOST'] = -9\n",
    "df_veh['VEHMILES'] = -9\n",
    "df_veh = df_veh[['HOUSEID','VEHID','BESTMILE','EIADMPG','GSCOST','VEHTYPE','VEHYEAR','VEHMILES']]\n",
    "# df_veh['HOUSEID']  = df_veh['HOUSEID'].apply(lambda x: \"'\" + str(x) + \"'\")\n",
    "list = [['BESTMILE',float],['EIADMPG',float],['GSCOST',float]]\n",
    "for item, format in list:\n",
    "    df_veh[item] = df_veh[item].astype(format)\n",
    "df_veh.to_csv('Veh_df.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 Hh_df - This module must be run together and AFTER Per_df and Veh_df!!!\n",
    "df_hhhts = pd.read_csv(_join('HTS', 'SDRTS_Household_Data_20170731 TAZ.csv'))\n",
    "df_hhhts_mgra = pd.read_csv(_join('HTS', 'SDRTS_Household_Data_20170731 MGRA.csv'))\n",
    "df_area = pd.read_csv(_join(target_path, 'inputs', 'bzone_unprotected_area.csv'))\n",
    "df_mgra = pd.read_csv(_join(scenpath, '2016', 'input\\mgra13_based_input' + '2016'+ '.csv'))\n",
    "df_cbg = pd.read_csv(_join('HTS', 'mgra13_CT10BG.csv'))\n",
    "df_areatype = pd.read_csv(_join('HTS', 'areatype_mgra.csv'), engine='python')\n",
    "df_income = pd.read_csv(_join('NHTS2001 analysis', 'hhincome share.csv'))\n",
    "\n",
    "df_countdriver = df_per.groupby(['SAMPN','license'])['personid'].count().reset_index()\n",
    "df_countdriver = df_countdriver.loc[df_countdriver['license'] == 1.0]\n",
    "df_countdriver = df_countdriver.rename(columns={'SAMPN': 'hhid', 'personid': 'DRVRCNT'})\n",
    "df_hhhts = df_hhhts.merge(df_countdriver[['hhid', 'DRVRCNT']], on='hhid', how='left')\n",
    "df_housecount = df_hhhts.groupby('home_address_TAZ')['hh_final_weight_456x'].sum().reset_index()\n",
    "df_area = df_area.loc[df_area['Year'] == 2016]\n",
    "df_area['area'] = (df_area['RuralArea']+df_area['TownArea']+df_area['UrbanArea']) * 0.0015625\n",
    "df_mgraTAZ = df_mgra.groupby('taz')['hs'].sum().reset_index()\n",
    "df_areaHh = df_area.merge(df_mgraTAZ, left_on='Geo', right_on='taz', how='left')\n",
    "df_per = df_per.merge(df_hhhts[['hhid','home_address_MGRA','hh_final_weight_456x']], left_on='SAMPN', right_on='hhid', how='left')\n",
    "\n",
    "df_mgra = df_mgra.merge(df_cbg, left_on='mgra', right_on='MGRA', how='left' )\n",
    "df_mgraden = df_mgra.groupby('CT10BG')['hs', 'pop', 'land_acres'].sum().reset_index()\n",
    "df_mgraden['popden'] = df_mgraden['pop']/(df_mgraden['land_acres'] * 0.0015625)\n",
    "df_mgraden['popden'] = df_mgraden['popden'].fillna(0)\n",
    "areaPer_bin = [0, 99,499,999,1999,3999,9999,24999,1000000000000] \n",
    "areaPer_labels = [50,300,750,1500,3000,7000,17000,30000]\n",
    "df_mgraden['HBPPOPDN']  = pd.cut(df_mgraden['popden'], bins=areaPer_bin, labels=areaPer_labels, include_lowest=True)\n",
    "df_mgraden['hsden'] = df_mgraden['hs']/(df_mgraden['land_acres'] * 0.0015625)\n",
    "df_mgraden['hsden'] = df_mgraden['hsden'].fillna(0)\n",
    "area_bin = [0, 49,249,999,2999,4999,100000000] \n",
    "area_labels = [25,150,700,2000,4000,6000]\n",
    "df_mgraden['HBHRESDN'] = pd.cut(df_mgraden['hsden'], bins=area_bin, labels=area_labels, include_lowest=True)\n",
    "df_mgra = df_mgra.merge(df_mgraden[['CT10BG','HBPPOPDN','HBHRESDN']], on='CT10BG', how='left')\n",
    "df_hhhts = df_hhhts.merge(df_mgra[['mgra', 'HBHRESDN', 'HBPPOPDN']], left_on='home_address_MGRA', right_on='mgra', how='left')\n",
    "df_hhhts = df_hhhts.merge(df_areatype[['MGRA', 'HBHUR']], left_on='home_address_MGRA', right_on='MGRA', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 Imputation of household income\n",
    "hhsize_bin = [0,1,3,100] \n",
    "hhsize_labels = ['A','B','C']\n",
    "df_hhhts['hhsizeGrp'] = pd.cut(df_hhhts['hhsize'], bins=hhsize_bin, labels=hhsize_labels, include_lowest=True)\n",
    "wksize_bin = [0,0.1,1,100] \n",
    "wksize_labels = ['0','1','2+']\n",
    "df_hhhts['wksizeGrp'] = pd.cut(df_hhhts['numworkers'], bins=wksize_bin, labels=wksize_labels, include_lowest=True)\n",
    "df_hhhts['hhsizeGrp'] = df_hhhts['hhsizeGrp'].astype(str)+'-'+df_hhhts['wksizeGrp'].astype(str)\n",
    "df_hhhts = df_hhhts.merge(df_income, left_on = 'hhsizeGrp', right_on='condition', how='left')\n",
    "df_hhhts['hhincome_imputed'] = df_hhhts['hhincome_imputed'].fillna(df_hhhts['hhincome_broad'])\n",
    "for i in range(len(df_hhhts)): # convert property list from string\n",
    "    df_hhhts['group1'][i] = ast.literal_eval(df_hhhts['group1'][i])\n",
    "    df_hhhts['group2'][i] = ast.literal_eval(df_hhhts['group2'][i])\n",
    "    df_hhhts['group3'][i] = ast.literal_eval(df_hhhts['group3'][i])\n",
    "df_hhhts['hhincome_imputed2'] = 0\n",
    "for i in range(len(df_hhhts)): # imputation\n",
    "    if df_hhhts['hhincome_imputed'][i] == 1:\n",
    "        df_hhhts['hhincome_imputed2'][i] = bisect(df_hhhts['group1'][i], random.random()) + 1\n",
    "    elif df_hhhts['hhincome_imputed'][i] == 2:\n",
    "        df_hhhts['hhincome_imputed2'][i] = bisect(df_hhhts['group2'][i], random.random()) + 7  \n",
    "    elif df_hhhts['hhincome_imputed'][i] == 3:\n",
    "        df_hhhts['hhincome_imputed2'][i] = bisect(df_hhhts['group3'][i], random.random()) + 13      \n",
    "    elif df_hhhts['hhincome_imputed'][i] in [4,5]:\n",
    "        df_hhhts['hhincome_imputed2'][i] = 18\n",
    "    elif df_hhhts['hhincome_imputed'][i] == 99:\n",
    "        df_hhhts['hhincome_imputed2'][i] = -9\n",
    "    else:\n",
    "        print ('exception', df_hhhts['hhid'][i], df_hhhts['hhincome_imputed'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7\n",
    "df_per2 = df_per.loc[df_per['PERNO']==1] # approximation, use the first person in HTS households as the respondent\n",
    "df_hhhts = df_hhhts.merge(df_per2[['SAMPN', 'R_AGE', 'license', 'HHR_RACE','gender']], left_on='hhid', right_on='SAMPN', how='left')\n",
    "df_veh2 = df_veh.groupby('HOUSEID')['VEHID'].count().reset_index()\n",
    "#df_hhhts['hhid']  = df_hhhts['hhid'].apply(lambda x: \"'\" + str(x) + \"'\")\n",
    "df_hhhts =df_hhhts.merge(df_veh2, left_on='hhid', right_on='HOUSEID', how='left')\n",
    "df_hhhts['res_type'] = df_hhhts['res_type'].replace({3: 4, 97: 91})\n",
    "# impute HTS 2-Townhouse (attached house) to NHTS 2-Duplex 6 and 3-Rowhouse or townhouse 6 [0.537, 0.463]\n",
    "row_list =  [0.537, 1]\n",
    "for i in range(len(df_hhhts)): # imputation\n",
    "    if df_hhhts['res_type'][i] == 2:\n",
    "        df_hhhts['res_type'][i] = bisect(row_list, random.random()) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 assign life cycle, adult: 18 and plus\n",
    "df_per['LIF_CYC'] = 0\n",
    "for hhid in df_per['SAMPN'].unique():\n",
    "    adult = adult_retire = child5 = child15 = child17 = 0\n",
    "    df_select = df_per.loc[df_per['SAMPN'] == hhid]\n",
    "    adult = len(df_select.loc[(df_select['R_AGE']>=18) & (df_select['EMPLY']!=4)]) \n",
    "    adult_retire = len(df_select.loc[(df_select['R_AGE']>=18) & (df_select['EMPLY']==4)]) \n",
    "    child5 = len(df_select.loc[df_select['R_AGE']<=5])\n",
    "    child15 = len(df_select.loc[(df_select['R_AGE']>=5) & (df_select['R_AGE']<=15)])\n",
    "    child17 = len(df_select.loc[(df_select['R_AGE']>=16) & (df_select['R_AGE']<=17)])\n",
    "    if adult_retire == 1 and adult == 0 and child5+child15+child17==0:\n",
    "        df_per.loc[df_per['SAMPN'] == hhid, ['LIF_CYC']] = 9\n",
    "    elif adult_retire == 2 and adult == 0 and child5+child15+child17==0:\n",
    "        df_per.loc[df_per['SAMPN'] == hhid, ['LIF_CYC']] = 10\n",
    "    else:\n",
    "        if adult + adult_retire == 1:\n",
    "            if child5 > 0:\n",
    "                df_per.loc[df_per['SAMPN'] == hhid, ['LIF_CYC']] = 3\n",
    "            elif child15 > 0:\n",
    "                df_per.loc[df_per['SAMPN'] == hhid, ['LIF_CYC']] = 5\n",
    "            elif child17 > 0:\n",
    "                df_per.loc[df_per['SAMPN'] == hhid, ['LIF_CYC']] = 7\n",
    "            else:\n",
    "                df_per.loc[df_per['SAMPN'] == hhid, ['LIF_CYC']] = 1\n",
    "        else:\n",
    "            if child5 > 0:\n",
    "                df_per.loc[df_per['SAMPN'] == hhid, ['LIF_CYC']] = 4\n",
    "            elif child15 > 0:\n",
    "                df_per.loc[df_per['SAMPN'] == hhid, ['LIF_CYC']] = 6\n",
    "            elif child17 > 0:\n",
    "                df_per.loc[df_per['SAMPN'] == hhid, ['LIF_CYC']] = 8\n",
    "            else:\n",
    "                df_per.loc[df_per['SAMPN'] == hhid, ['LIF_CYC']] = 2\n",
    "df_per3 = df_per[df_per['PERNO'] == 1]\n",
    "df_hhhts = df_hhhts.merge(df_per3[['SAMPN','LIF_CYC']], on ='SAMPN', how ='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9\n",
    "df_hhhts['MSAPOP'] = [df_per['hh_final_weight_456x'].sum()] * len(df_hhhts)\n",
    "df_per4 = df_per2.loc[df_per['R_AGE']>=16]\n",
    "df_per5 = df_per4.groupby('SAMPN')['PERNO'].count().reset_index().rename(columns={'PERNO': 'PER16+'}) \n",
    "df_hhhts = df_hhhts.merge(df_per5[['SAMPN','PER16+']], on='SAMPN', how='left')\n",
    "df_hhhts['RATIO16V'] = df_hhhts['PER16+'] / df_hhhts['VEHID']\n",
    "df_hhhts['URBAN'] = 2\n",
    "df_hhhts.loc[df_hhhts['HBHUR']=='R', 'URBAN'] = 4\n",
    "df_hhhts['URBRUR'] = 1\n",
    "df_hhhts.loc[df_hhhts['HBHUR']=='R', 'URBRUR'] = 2\n",
    "df_per6 = df_per.loc[df_per['WORKER']==1]\n",
    "df_per7 = df_per6.groupby('SAMPN')['PERNO'].count().reset_index().rename(columns={'PERNO': 'WRKCOUNT'}) \n",
    "df_hhhts = df_hhhts.merge(df_per7[['SAMPN','WRKCOUNT']], on='SAMPN', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 assign age related AGE_, DRV_, WKR_\n",
    "df_perage = df_per.pivot_table(values='R_AGE', index=['SAMPN'], columns=['PERNO'], aggfunc=np.sum)\n",
    "df_perage = df_perage.fillna(-1)\n",
    "df_perage[13] = df_perage[14] = -1\n",
    "df_perage = df_perage.astype(int)\n",
    "df_perage.columns = ['AGE_P1','AGE_P2','AGE_P3','AGE_P4','AGE_P5','AGE_P6','AGE_P7','AGE_P8','AGE_P9','AGE_P10','AGE_P11','AGE_P12','AGE_P13','AGE_P14']\n",
    "\n",
    "df_perdr = df_per.pivot_table(values='license', index=['SAMPN'], columns=['PERNO'], aggfunc=np.sum)\n",
    "df_perdr = df_perdr.fillna(-1)\n",
    "df_perdr[13] = df_perdr[14] = -1\n",
    "df_perdr = df_perdr.astype(int)\n",
    "df_perdr.columns = ['DRV_P1','DRV_P2','DRV_P3','DRV_P4','DRV_P5','DRV_P6','DRV_P7','DRV_P8','DRV_P9','DRV_P10','DRV_P11','DRV_P12','DRV_P13','DRV_P14']\n",
    "\n",
    "df_perwr = df_per.pivot_table(values='WORKER', index=['SAMPN'], columns=['PERNO'], aggfunc=np.sum)\n",
    "df_perwr = df_perwr.fillna(-1)\n",
    "df_perwr[13] = df_perwr[14] = -1\n",
    "df_perwr = df_perwr.astype(int)\n",
    "df_perwr.columns = ['WKR_P1','WKR_P2','WKR_P3','WKR_P4','WKR_P5','WKR_P6','WKR_P7','WKR_P8','WKR_P9','WKR_P10','WKR_P11','WKR_P12','WKR_P13','WKR_P14']\n",
    "df_perall = df_perage.merge(df_perdr, on='SAMPN', how='inner')\n",
    "df_perall = df_perall.merge(df_perwr, on='SAMPN', how='inner')\n",
    "df_hhhts = df_hhhts.merge(df_perall, on='SAMPN', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11 assign MSACAT\n",
    "df_SDMSA = pd.read_csv(_join(r'T:\\ABM\\ABM_FY22\\RSM\\VisionEval\\Model\\Update_Automations\\SX', 'mgra13_msacat.csv'))\n",
    "df_hhhts = df_hhhts.merge(df_SDMSA[['mgra', 'Msacat', 'HHC_MSA']], left_on='home_address_MGRA', right_on='mgra', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12 approximations/placeholders\n",
    "df_hhhts['FLGFINCM'] = 1 # assume incomes of all HH members are included\n",
    "df_hhhts['CENSUS_D'] = 9\n",
    "df_hhhts['CENSUS_R'] = 4\n",
    "# df_hhhts['HHC_MSA'] = '7320'\n",
    "df_hhhts['HHINCTTL'] = df_hhhts['hhincome_imputed2']\n",
    "df_hhhts['HHNUMBIK'] = 0\n",
    "df_hhhts['HTEEMPDN'] = 0\n",
    "df_hhhts['HTHRESDN'] = df_hhhts['HBHRESDN']\n",
    "df_hhhts['HTHUR'] = df_hhhts['HBHUR']\n",
    "df_hhhts['HTPPOPDN'] = df_hhhts['HBPPOPDN']\n",
    "#df_hhhts['MSACAT'] = 2\n",
    "df_hhhts['MSASIZE'] = 5\n",
    "df_hhhts['RAIL'] = 2\n",
    "df_hhhts['CNTTDHH'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13\n",
    "df_hhhts2 = df_hhhts[['hhid','AGE_P1','AGE_P2','AGE_P3','AGE_P4','AGE_P5','AGE_P6','AGE_P7','AGE_P8','AGE_P9','AGE_P10',\\\n",
    "                     'AGE_P11','AGE_P12','AGE_P13','AGE_P14','CENSUS_D','CENSUS_R','DRVRCNT','DRV_P1','DRV_P2','DRV_P3',\\\n",
    "                     'DRV_P4','DRV_P5','DRV_P6','DRV_P7','DRV_P8','DRV_P9','DRV_P10','DRV_P11','DRV_P12','DRV_P13',\\\n",
    "                     'DRV_P14','hh_final_weight_456x','hh_final_weight_456x','FLGFINCM','HBHRESDN','HBHUR','HBPPOPDN',\\\n",
    "                     'HHC_MSA','hhincome_imputed2','HHINCTTL','HHNUMBIK','R_AGE','license','HHR_RACE','gender','hhsize',\\\n",
    "                     'VEHID','res_type','HTEEMPDN','HTHRESDN','HTHUR','HTPPOPDN','LIF_CYC','MSAPOP','Msacat','MSASIZE',\\\n",
    "                     'RAIL','RATIO16V','URBAN','URBRUR','WRKCOUNT','WKR_P1','WKR_P2','WKR_P3','WKR_P4','WKR_P5','WKR_P6',\\\n",
    "                     'WKR_P7','WKR_P8','WKR_P9','WKR_P10','WKR_P11','WKR_P12','WKR_P13','WKR_P14','CNTTDHH']]\n",
    "df_hhhts2.columns = ['HOUSEID','AGE_P1','AGE_P2','AGE_P3','AGE_P4','AGE_P5','AGE_P6','AGE_P7','AGE_P8','AGE_P9','AGE_P10',\\\n",
    "                     'AGE_P11','AGE_P12','AGE_P13','AGE_P14','CENSUS_D','CENSUS_R','DRVRCNT','DRV_P1','DRV_P2','DRV_P3',\\\n",
    "                     'DRV_P4','DRV_P5','DRV_P6','DRV_P7','DRV_P8','DRV_P9','DRV_P10','DRV_P11','DRV_P12','DRV_P13',\\\n",
    "                     'DRV_P14','EXPFLHHN','EXPFLLHH','FLGFINCM','HBHRESDN','HBHUR','HBPPOPDN','HHC_MSA','HHFAMINC',\\\n",
    "                     'HHINCTTL','HHNUMBIK','HHR_AGE','HHR_DRVR','HHR_RACE','HHR_SEX','HHSIZE','HHVEHCNT','HOMETYPE',\\\n",
    "                     'HTEEMPDN','HTHRESDN','HTHUR','HTPPOPDN','LIF_CYC','MSAPOP','MSACAT','MSASIZE','RAIL','RATIO16V',\\\n",
    "                     'URBAN','URBRUR','WRKCOUNT','WKR_P1','WKR_P2','WKR_P3','WKR_P4','WKR_P5','WKR_P6','WKR_P7','WKR_P8',\\\n",
    "                     'WKR_P9','WKR_P10','WKR_P11','WKR_P12','WKR_P13','WKR_P14','CNTTDHH']\n",
    "# df_hhhts2['HHC_MSA']  = df_hhhts2['HHC_MSA'].apply(lambda x: \"'\" + str(x) + \"'\")\n",
    "df_hhhts2['DRVRCNT'] = df_hhhts2['DRVRCNT'].fillna(0)\n",
    "list = [['DRVRCNT',int],['HHR_DRVR',int],['HHVEHCNT',int],['MSAPOP',int],['WRKCOUNT',int]]\n",
    "for item, format in list:\n",
    "    df_hhhts2[item] = df_hhhts2[item].fillna(0)\n",
    "    df_hhhts2[item] = df_hhhts2[item].astype(format)\n",
    "df_hhhts2.to_csv('Hh_df.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign unique tourID to RSG Tour file\n",
    "df_tour = pd.read_csv(_join('HTS', 'tours_debug.csv'))\n",
    "df = pd.DataFrame()\n",
    "for hhid in df_tour['HH_ID'].unique():\n",
    "    for perid in df_tour.loc[(df_tour['HH_ID'] == hhid)]['PER_ID'].unique():\n",
    "        df_select = df_tour.loc[(df_tour['HH_ID'] == hhid) & (df_tour['PER_ID'] == perid)]\n",
    "        l = [i for i in range(1,len(df_select)+1,1)]\n",
    "        df_select = df_select.set_index([pd.Index(l)])\n",
    "        df_select = df_select.reset_index().rename({'index':'TOUR_ID2'}, axis = 'columns')\n",
    "        df =df.append(df_select)\n",
    "df.to_csv('tours_debug_newid.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign unique tourID to RSG Trip File\n",
    "df_trip = pd.read_csv(_join('HTS', 'trips_debug.csv'))\n",
    "df_ntrip = pd.DataFrame()\n",
    "#j=0\n",
    "for hhid in df_trip['HH_ID'].unique():\n",
    "    #if hhid == 161000914:\n",
    "        for perid in df_trip.loc[(df_trip['HH_ID'] == hhid)]['PER_ID'].unique():        \n",
    "            df_selectTrip = df_trip.loc[(df_trip['HH_ID'] == hhid) & (df_trip['PER_ID'] == perid)]\n",
    "            #print ('df_selectTrip aaa')\n",
    "            #print (df_selectTrip[['HH_ID','PER_ID','TOUR_ID','TRIP_ID','ORIG_PLACENO']], len(df_selectTrip))\n",
    "            l = [i for i in range(0,len(df_selectTrip),1)]\n",
    "            df_selectTrip = df_selectTrip.set_index([pd.Index(l)])\n",
    "            tourid = 0\n",
    "            for i in range(len(df_selectTrip)):\n",
    "                if df_selectTrip.iloc[i]['TRIP_ID'] == 1:\n",
    "                    tourid = tourid + 1\n",
    "                    #print ('if i, tourid', i, tourid)\n",
    "                    #print ('df_selectTrip.iloc[i]', df_selectTrip.iloc[i])\n",
    "                    #print ('df_selectTrip ccc')\n",
    "                    #print (df_selectTrip[['HH_ID','PER_ID','TOUR_ID','TRIP_ID','ORIG_PLACENO']])       \n",
    "                #print ('else i, tourid', i, tourid)\n",
    "                df_selectTrip.at[i,'TOUR_ID'] = tourid\n",
    "                #print ('df_selectTrip ddd')\n",
    "                #print (df_selectTrip[['HH_ID','PER_ID','TOUR_ID','TRIP_ID','ORIG_PLACENO']])                       \n",
    "                #print ('i', i)\n",
    "            #print ('df_selectTrip bbb, i', i)\n",
    "            #print (df_selectTrip[['HH_ID','PER_ID','TOUR_ID','TRIP_ID','ORIG_PLACENO']])                \n",
    "            df_ntrip = df_ntrip.append(df_selectTrip)\n",
    "    #else:\n",
    "    #    continue\n",
    "    #j += 1\n",
    "df_ntrip.to_csv('trips_debug_newid', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
